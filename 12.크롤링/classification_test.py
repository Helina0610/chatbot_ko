# ì„¤ì¹˜ í•„ìš” ì‹œ
# pip install sentence-transformers scikit-learn

from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import numpy as np


# 1. ë¬¸ì„œ ë¶„í•  í•¨ìˆ˜
def split_document(text: str, max_tokens: int = 200) -> list[str]:
    sentences = text.split(". ")
    chunks = []
    current_chunk = []
    current_length = 0

    for sentence in sentences:
        token_length = len(sentence.split())
        if current_length + token_length > max_tokens:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_length = token_length
        else:
            current_chunk.append(sentence)
            current_length += token_length

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks


# 2. ë¬¸ì„œ ì„ë² ë”© í•¨ìˆ˜ (í‰ê·  ì„ë² ë”© ë°©ì‹)
def get_document_embedding(text: str, model: SentenceTransformer) -> np.ndarray:
    chunks = split_document(text)
    if not chunks:
        return np.zeros(model.get_sentence_embedding_dimension())

    embeddings = model.encode(chunks)
    return np.mean(embeddings, axis=0)


# 3. ì˜ˆì‹œ ë°ì´í„° (ì‹¤ì œ ë°ì´í„°ë¡œ êµì²´í•˜ì„¸ìš”)
documents = [
    "ì´ ë¬¸ì„œëŠ” ê³ ì–‘ì´ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê³ ì–‘ì´ëŠ” í¬ìœ ë¥˜ì…ë‹ˆë‹¤. ì•¼í–‰ì„±ì´ë©° ë…ë¦½ì ì¸ ì„±ê²©ì„ ê°€ì§‘ë‹ˆë‹¤.",
    "ì´ ë¬¸ì„œëŠ” ê°•ì•„ì§€ì— ê´€í•œ ë‚´ìš©ì…ë‹ˆë‹¤. ê°•ì•„ì§€ëŠ” ì‚¬ëŒê³¼ ì¹œìˆ™í•˜ë©° ì¶©ì„±ì‹¬ì´ ê°•í•©ë‹ˆë‹¤. ì• ì™„ë™ë¬¼ë¡œ ë§ì´ ê¸¸ëŸ¬ì§‘ë‹ˆë‹¤.",
    "ê°•ì•„ì§€ì™€ ê³ ì–‘ì´ëŠ” ì¸ê¸° ìˆëŠ” ë°˜ë ¤ë™ë¬¼ì…ë‹ˆë‹¤. ê°ê°ì˜ íŠ¹ì§•ì´ ë‹¤ë¥´ë©°, ì‚¬ëŒë“¤ì˜ ì·¨í–¥ì— ë”°ë¼ ì„ íƒë©ë‹ˆë‹¤.",
    "ê³ ì–‘ì´ëŠ” ê¹¨ë—í•œ ë™ë¬¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ìŠ¤ìŠ¤ë¡œ ê·¸ë£¨ë°ì„ í•˜ë©°, ì¡°ìš©í•œ ì„±ê²©ì„ ê°€ì¡ŒìŠµë‹ˆë‹¤.",
    "ê°•ì•„ì§€ëŠ” ì‚°ì±…ì„ ì¢‹ì•„í•˜ë©° ì£¼ì¸ì—ê²Œ ì• ì •ì„ ìì£¼ í‘œí˜„í•©ë‹ˆë‹¤. êµìœ¡ì´ ìƒëŒ€ì ìœ¼ë¡œ ìš©ì´í•©ë‹ˆë‹¤."
]
labels = [0, 1, 1, 0, 1]  # 0: ê³ ì–‘ì´, 1: ê°•ì•„ì§€ (ì˜ˆì‹œ)

# 4. ëª¨ë¸ ë¡œë”©
model = SentenceTransformer("all-MiniLM-L6-v2")

# 5. ë¬¸ì„œ ì„ë² ë”© ì¶”ì¶œ
X = [get_document_embedding(doc, model) for doc in documents]
y = labels

# 6. ë¶„ë¥˜ê¸° í•™ìŠµ
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = LogisticRegression()
clf.fit(X_train, y_train)

# 7. í‰ê°€
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred, target_names=["ê³ ì–‘ì´", "ê°•ì•„ì§€"]))

#import os
#from glob import glob
#from langchain_community.document_loaders import PyPDFLoader
#import numpy as np
#from sklearn.linear_model import LogisticRegression
#from sklearn.metrics import classification_report
#from sklearn.model_selection import train_test_split
#from sentence_transformers import SentenceTransformer

## 1. ë¬¸ì„œ ë¶„í•  í•¨ìˆ˜
#def split_document(text: str, max_tokens: int = 200) -> list[str]:

#    sentences = text.split(". ")
#    chunks = []
#    current_chunk = []
#    current_length = 0

#    for sentence in sentences:
#        token_length = len(sentence.split())
#        if current_length + token_length > max_tokens:
#            chunks.append(" ".join(current_chunk))
#            current_chunk = [sentence]
#            current_length = token_length
#        else:
#            current_chunk.append(sentence)
#            current_length += token_length

#    if current_chunk:
#        chunks.append(" ".join(current_chunk))

#    return chunks
  
## 2. ë¬¸ì„œ ì„ë² ë”© í•¨ìˆ˜ (í‰ê·  ì„ë² ë”© ë°©ì‹)
#def get_document_embedding(text: str, model: SentenceTransformer) -> np.ndarray:
#    chunks = split_document(text)
#    if not chunks:
#        return np.zeros(model.get_sentence_embedding_dimension())

#    embeddings = model.encode(chunks)
#    return np.mean(embeddings, axis=0)


#LOCAL_MODEL_PATH = "local_model/multilingual-e5-small-ko"
#model = SentenceTransformer(LOCAL_MODEL_PATH)

## íŒŒì¼ ëª©ë¡
#labels = ["ì¸ì‚¬ìë£Œí†µë³´","ì£¼ì˜","ì§•ê³„ë¬¸ì±…","í†µë³´ê¶Œê³ "]
#docs = []
#pdf_files = glob(os.path.join('data/data', '*ì²˜ë¶„ìš”êµ¬ë³„ ê³µê°œë¬¸-*.pdf'))
#print(len(pdf_files))
#for pdf in pdf_files:
#    loader = PyPDFLoader(file_path=pdf, mode="single")
#    documents = loader.load()
#    for document in documents:  # documents ë‚´ë¶€ Document í•˜ë‚˜ì”© êº¼ë‚´ê¸°
#        docs.append(document)

#print(len(docs))
#for doc in docs:
#    X = [get_document_embedding(doc.page_content, model) for doc in docs]
#    Y = labels

#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

#clf = LogisticRegression()
#clf.fit(X_train, y_train)

## 7. í‰ê°€
#y_pred = clf.predict(X_test)
#print(classification_report(y_test, y_pred, target_names=["í†µë³´ê¶Œê³ ", "ì£¼ì˜"]))


#import os
#from glob import glob
#from langchain_community.document_loaders import PyPDFLoader
#import numpy as np
#from sklearn.linear_model import LogisticRegression
#from sklearn.metrics import classification_report
#from sklearn.model_selection import train_test_split
#from sentence_transformers import SentenceTransformer


## 1. ë¬¸ì„œ ë¶„í•  í•¨ìˆ˜
#def split_document(text: str, max_tokens: int = 200) -> list[str]:
#    sentences = text.split(". ")
#    chunks = []
#    current_chunk = []
#    current_length = 0

#    for sentence in sentences:
#        token_length = len(sentence.split())
#        if current_length + token_length > max_tokens:
#            chunks.append(" ".join(current_chunk))
#            current_chunk = [sentence]
#            current_length = token_length
#        else:
#            current_chunk.append(sentence)
#            current_length += token_length

#    if current_chunk:
#        chunks.append(" ".join(current_chunk))
#    return chunks


## 2. ë¬¸ì„œ ì„ë² ë”© í•¨ìˆ˜ (í‰ê·  ì„ë² ë”© ë°©ì‹)
#def get_document_embedding(text: str, model: SentenceTransformer) -> np.ndarray:
#    chunks = split_document(text)
#    if not chunks:
#        return np.zeros(model.get_sentence_embedding_dimension())
#    embeddings = model.encode(chunks)
#    return np.mean(embeddings, axis=0)


## 3. ëª¨ë¸ ë¡œë”©
#LOCAL_MODEL_PATH = "local_model/multilingual-e5-small-ko"
#model = SentenceTransformer(LOCAL_MODEL_PATH)


## 4. ë¬¸ì„œ ì„ë² ë”© ë° ë¼ë²¨ ì¶”ì¶œ
#X = []
#Y = []

#pdf_files = glob(os.path.join('data/data', '*ì²˜ë¶„ìš”êµ¬ë³„ ê³µê°œë¬¸-*.pdf'))
#print(f"PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}")

#for pdf in pdf_files:
#    loader = PyPDFLoader(file_path=pdf, mode="single")
#    documents = loader.load()

#    # ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì´ì–´ë¶™ì´ê¸°
#    full_text = " ".join([doc.page_content for doc in documents])
#    embedding = get_document_embedding(full_text, model)
#    if np.count_nonzero(embedding) == 0:
#        continue

#    X.append(embedding)

#    # íŒŒì¼ëª…ì—ì„œ ë¼ë²¨ ì¶”ì¶œ
#    filename = os.path.basename(pdf)
#    if "ì¸ì‚¬ìë£Œí†µë³´" in filename:
#        Y.append("ì¸ì‚¬ìë£Œí†µë³´")
#    elif "ì£¼ì˜" in filename:
#        Y.append("ì£¼ì˜")
#    elif "ì§•ê³„ë¬¸ì±…" in filename:
#        Y.append("ì§•ê³„ë¬¸ì±…")
#    elif "í†µë³´ê¶Œê³ " in filename:
#        Y.append("í†µë³´ê¶Œê³ ")
#    else:
#        print(f"âš ï¸ ë¼ë²¨ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ: {filename}")


#print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(X)} / ë¼ë²¨ ìˆ˜: {len(Y)}")

#if len(X) < 2:
#    print("âŒ í•™ìŠµí•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
#    exit()

## 5. ë¶„ë¥˜ê¸° í•™ìŠµ
#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
#clf = LogisticRegression(max_iter=1000)
#clf.fit(X_train, y_train)

#import joblib

## í•™ìŠµëœ ëª¨ë¸ ì €ì¥
#joblib.dump(clf, "classifier.pkl")
#print("âœ… LogisticRegression ëª¨ë¸ì´ classifier.pkl íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

#import joblib

## ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
#clf = joblib.load("classifier.pkl")
#print("âœ… ì €ì¥ëœ ë¶„ë¥˜ê¸° ëª¨ë¸ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

## ì˜ˆì¸¡ ê°€ëŠ¥
#y_pred = clf.predict([new_embedding])



## 6. í‰ê°€
#y_pred = clf.predict(X_test)
#print(classification_report(y_test, y_pred))



#def classify_new_document(pdf_path: str) -> str:
#    loader = PyPDFLoader(file_path=pdf_path, mode="single")
#    documents = loader.load()
#    full_text = " ".join([doc.page_content for doc in documents])
#    embedding = get_document_embedding(full_text, model)

#    if np.count_nonzero(embedding) == 0:
#        return "âŒ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

#    prediction = clf.predict([embedding])[0]
#    return prediction


#new_pdf = "data/test/ì²˜ë¶„ìš”êµ¬ë³„ ê³µê°œë¬¸-ìƒˆë¡œìš´ë¬¸ì„œ.pdf"
#predicted_label = classify_new_document(new_pdf)
#print(f"ì˜ˆì¸¡ëœ ë¼ë²¨: {predicted_label}")

#import os
#from glob import glob
#import joblib
#import numpy as np
#from langchain_community.document_loaders import PyPDFLoader
#from sentence_transformers import SentenceTransformer
#from sklearn.linear_model import LogisticRegression
#from sklearn.model_selection import train_test_split
#from sklearn.metrics import classification_report

## ë¬¸ì„œ ë¶„í•  í•¨ìˆ˜
#def split_document(text: str, max_tokens: int = 200) -> list[str]:
#    sentences = text.split(". ")
#    chunks, current_chunk, current_length = [], [], 0
#    for sentence in sentences:
#        token_length = len(sentence.split())
#        if current_length + token_length > max_tokens:
#            chunks.append(" ".join(current_chunk))
#            current_chunk = [sentence]
#            current_length = token_length
#        else:
#            current_chunk.append(sentence)
#            current_length += token_length
#    if current_chunk:
#        chunks.append(" ".join(current_chunk))
#    return chunks

## ì„ë² ë”© í•¨ìˆ˜
#def get_document_embedding(text: str, model: SentenceTransformer) -> np.ndarray:
#    chunks = split_document(text)
#    if not chunks:
#        return np.zeros(model.get_sentence_embedding_dimension())
#    embeddings = model.encode(chunks)
#    return np.mean(embeddings, axis=0)

## PDF â†’ ì„ë² ë”© + ë¼ë²¨ ì¶”ì¶œ
#def build_dataset(pdf_files: list[str], model: SentenceTransformer):
#    X, Y = [], []
#    for pdf in pdf_files:
#        loader = PyPDFLoader(file_path=pdf, mode="single")
#        documents = loader.load()
#        full_text = " ".join([doc.page_content for doc in documents])
#        embedding = get_document_embedding(full_text, model)
#        if np.count_nonzero(embedding) == 0:
#            continue
#        X.append(embedding)

#        # ë¼ë²¨ ì¶”ì¶œ
#        filename = os.path.basename(pdf)
#        if "ì¸ì‚¬ìë£Œí†µë³´" in filename:
#            Y.append("ì¸ì‚¬ìë£Œí†µë³´")
#        elif "ì£¼ì˜" in filename:
#            Y.append("ì£¼ì˜")
#        elif "ì§•ê³„ë¬¸ì±…" in filename:
#            Y.append("ì§•ê³„ë¬¸ì±…")
#        elif "í†µë³´ê¶Œê³ " in filename:
#            Y.append("í†µë³´ê¶Œê³ ")
#        else:
#            print(f"[ê²½ê³ ] ë¼ë²¨ ì¸ì‹ ì‹¤íŒ¨: {filename}")
#    return X, Y

## ë¬¸ì„œ ë¶„ë¥˜ í•¨ìˆ˜ (ìƒˆ ë¬¸ì„œ)
#def classify_new_document(pdf_path: str, model: SentenceTransformer, clf, label_list):
#    loader = PyPDFLoader(file_path=pdf_path, mode="single")
#    documents = loader.load()
#    full_text = " ".join([doc.page_content for doc in documents])
#    embedding = get_document_embedding(full_text, model)
#    if np.count_nonzero(embedding) == 0:
#        return "âŒ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#    pred = clf.predict([embedding])[0]
#    return pred

## ---------- ë©”ì¸ íŒŒì´í”„ë¼ì¸ ----------
#if __name__ == "__main__":
#    LOCAL_MODEL_PATH = "local_model/multilingual-e5-small-ko"
#    EMBEDDING_MODEL = SentenceTransformer(LOCAL_MODEL_PATH)

#    # Step 1: í•™ìŠµìš© PDF ì½ê¸°
#    pdf_files = glob(os.path.join("data/data", "*ì²˜ë¶„ìš”êµ¬ë³„ ê³µê°œë¬¸-*.pdf"))
#    print(f"í•™ìŠµìš© PDF ìˆ˜: {len(pdf_files)}")

#    # Step 2: ì„ë² ë”© + ë¼ë²¨ ì¶”ì¶œ
#    X, Y = build_dataset(pdf_files, EMBEDDING_MODEL)
#    print(f"ì„ë² ë”©ëœ ë¬¸ì„œ ìˆ˜: {len(X)}, ë¼ë²¨ ìˆ˜: {len(Y)}")

#    # Step 3: ë¶„ë¥˜ê¸° í•™ìŠµ
#    if len(X) < 2:
#        print("âŒ í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
#        exit()

#    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
#    clf = LogisticRegression(max_iter=1000)
#    clf.fit(X_train, y_train)

#    # Step 4: í‰ê°€
#    y_pred = clf.predict(X_test)
#    print("\nğŸ“Š ë¶„ë¥˜ ì„±ëŠ¥:")
#    print(classification_report(y_test, y_pred))

#    # Step 5: ëª¨ë¸ ì €ì¥
#    joblib.dump(clf, "classifier.pkl")
#    joblib.dump(clf.classes_, "labels.pkl")
#    print("\nâœ… ëª¨ë¸ê³¼ ë¼ë²¨ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

#    # Step 6: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ë¶„ë¥˜ ì˜ˆì‹œ
#    test_pdf = "data/test/ì²˜ë¶„ìš”êµ¬ë³„ ê³µê°œë¬¸-í…ŒìŠ¤íŠ¸.pdf"  # ì˜ˆì‹œ íŒŒì¼ ê²½ë¡œ
#    if os.path.exists(test_pdf):
#        clf_loaded = joblib.load("classifier.pkl")
#        label_list = joblib.load("labels.pkl")
#        result = classify_new_document(test_pdf, EMBEDDING_MODEL, clf_loaded, label_list)
#        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì˜ˆì¸¡ ê²°ê³¼: {result}")
#    else:
#        print("âš ï¸ í…ŒìŠ¤íŠ¸ ë¬¸ì„œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

#import os
#from glob import glob
#from tqdm import tqdm
#import json
#import nltk
#import torch
#from transformers import AutoTokenizer
#from langchain_community.document_loaders import PyPDFLoader

#nltk.download('punkt')

## âœ… ì„¤ì •
#MODEL_PATH = "local_model/multilingual-sentiment-analysis"
#PDF_DIR = "data/data"
#MAX_TOKENS = 512
#STRIDE = 256
#MIN_TOKENS = 10

## âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ
#tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

## âœ… ë¬¸ì¥ ê¸°ë°˜ ì²­í¬ ìƒì„± í•¨ìˆ˜
#def sentence_based_chunk(text, max_tokens=MAX_TOKENS, stride=STRIDE):
#    sentences = nltk.sent_tokenize(text)
#    chunks, current_chunk = [], []
#    current_length = 0

#    for sent in sentences:
#        token_length = len(tokenizer.tokenize(sent))
#        if token_length > max_tokens:
#            continue  # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ë²„ë¦¼
#        if current_length + token_length > max_tokens:
#            chunks.append(" ".join(current_chunk))
#            current_chunk = [sent]
#            current_length = token_length
#        else:
#            current_chunk.append(sent)
#            current_length += token_length

#    if current_chunk:
#        chunks.append(" ".join(current_chunk))

#    return chunks

## âœ… ë¼ë²¨ ì¶”ì¶œ í•¨ìˆ˜ (íŒŒì¼ëª… ê¸°ë°˜)
#def extract_label_from_filename(filename):
#    if "ì¸ì‚¬ìë£Œí†µë³´" in filename:
#        return "ì¸ì‚¬ìë£Œí†µë³´"
#    elif "ì£¼ì˜" in filename:
#        return "ì£¼ì˜"
#    elif "ì§•ê³„ë¬¸ì±…" in filename:
#        return "ì§•ê³„ë¬¸ì±…"
#    elif "í†µë³´ê¶Œê³ " in filename:
#        return "í†µë³´ê¶Œê³ "
#    return None

## âœ… PDF ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í¬ ì „ì²˜ë¦¬
#examples = []
#pdf_files = glob(os.path.join(PDF_DIR, "*ì²˜ë¶„ìš”êµ¬ë³„ ê³µê°œë¬¸-*.pdf"))

#for pdf in tqdm(pdf_files):
#    label = extract_label_from_filename(pdf)
#    if not label:
#        continue

#    loader = PyPDFLoader(file_path=pdf, mode="single")
#    documents = loader.load()
#    full_text = " ".join([doc.page_content for doc in documents])

#    chunks = sentence_based_chunk(full_text)
#    for chunk in chunks:
#        tokens = tokenizer.encode(chunk, truncation=True, max_length=MAX_TOKENS)
#        if len(tokens) >= MIN_TOKENS:
#            examples.append({
#                "text": chunk,
#                "label": label
#            })

## âœ… ê²°ê³¼ ì €ì¥
#data_path = "preprocessed_data.jsonl"
#with open(data_path, "w", encoding="utf-8") as f:
#    for ex in examples:
#        f.write(json.dumps(ex, ensure_ascii=False) + "\n")

#print(f"âœ… ì´ {len(examples)}ê±´ ì²­í¬ ì €ì¥ ì™„ë£Œ: {data_path}")

##âœ… 1. HuggingFace datasetsë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
#from datasets import load_dataset

#dataset = load_dataset("json", data_files="preprocessed_data.jsonl", split="train")
#label_list = sorted(set(example['label'] for example in dataset))
#label2id = {label: idx for idx, label in enumerate(label_list)}
#id2label = {v: k for k, v in label2id.items()}

#def tokenize_fn(ex):
#    return tokenizer(ex["text"], truncation=True, padding="max_length", max_length=512)

#def encode_labels(ex):
#    ex["label"] = label2id[ex["label"]]
#    return ex

#tokenized_ds = dataset.map(tokenize_fn).map(encode_labels)

## âœ… 2. Trainerë¡œ íŒŒì¸íŠœë‹
#from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

#model = AutoModelForSequenceClassification.from_pretrained(
#    MODEL_PATH, num_labels=len(label_list), id2label=id2label, label2id=label2id
#)

#training_args = TrainingArguments(
#    output_dir="./results",
#    evaluation_strategy="no",
#    save_strategy="epoch",
#    learning_rate=2e-5,
#    per_device_train_batch_size=8,
#    num_train_epochs=3,
#    weight_decay=0.01,
#    logging_dir="./logs",
#    save_total_limit=2,
#)

#trainer = Trainer(
#    model=model,
#    args=training_args,
#    train_dataset=tokenized_ds,
#    tokenizer=tokenizer,
#)

#trainer.train()
#trainer.save_model("fine_tuned_model")

## âœ… 3. ì¶”ë¡ 
#from transformers import pipeline

#pipe = pipeline("text-classification", model="fine_tuned_model", tokenizer=tokenizer, top_k=1)

#text = "ê³µë¬´ì›ì˜ íƒœë„ëŠ” ì§•ê³„ë¬¸ì±… ì‚¬ìœ ê°€ ë©ë‹ˆë‹¤."
#pred = pipe(text)
#print(pred[0])

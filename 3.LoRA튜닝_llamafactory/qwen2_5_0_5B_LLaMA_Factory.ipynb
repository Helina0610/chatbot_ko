{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1F6HHrC8BOR",
        "outputId": "64f47a7a-74e2-490c-b844-2f56a4a64874"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WjiUM3JlbPC",
        "outputId": "766aa824-e792-47ec-cd05-438a227742da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 20060, done.\u001b[K\n",
            "remote: Counting objects: 100% (668/668), done.\u001b[K\n",
            "remote: Compressing objects: 100% (312/312), done.\u001b[K\n",
            "remote: Total 20060 (delta 536), reused 356 (delta 356), pack-reused 19392 (from 2)\u001b[K\n",
            "Receiving objects: 100% (20060/20060), 232.67 MiB | 19.53 MiB/s, done.\n",
            "Resolving deltas: 100% (14620/14620), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaMA-Factory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfLnPvbzlc5a",
        "outputId": "ce01e158-27cc-42f9-e85a-df20983e1bf0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqyegIMSmCcC",
        "outputId": "6e2f139f-2026-4625-e878-6d9c850d4e38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-6lNNB9leQ2",
        "outputId": "6c70571e-316b-4314-d036-9df63328e4c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers<=4.46.1,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.46.1)\n",
            "Requirement already satisfied: datasets<=3.1.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.12.0)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.9.6)\n",
            "Requirement already satisfied: tokenizers<0.20.4,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.20.3)\n",
            "Requirement already satisfied: gradio<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.44.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.8.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (4.25.5)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.34.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.10.3)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.115.6)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (2.2.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (3.8.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (1.26.4)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (14.0.1)\n",
            "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (0.8.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (0.27.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (3.11.10)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (2.5.1+cu121)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (3.10.12)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (10.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (0.8.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (2.2.3)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 8)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 8)) (2024.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->-r requirements.txt (line 14)) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->-r requirements.txt (line 14)) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->-r requirements.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->-r requirements.txt (line 15)) (2.27.1)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->-r requirements.txt (line 16)) (0.41.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (3.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 19)) (2.5.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->-r requirements.txt (line 24)) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->-r requirements.txt (line 24)) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->-r requirements.txt (line 24)) (1.7.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (1.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 24)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 24)) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 24)) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkvGW3pylqTP",
        "outputId": "c73f6940-634d-4eb3-ba5d-d56a9dc5f56f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3ONl_QxlwB0",
        "outputId": "2250183a-5255-43fd-eebd-cc98b8dfc768"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-rf4g57p5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-rf4g57p5\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit b2f2977533445c4f62bf58e10b1360e6856e78ce\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.0.dev0)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2024.12.14)\n",
            "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.48.0.dev0-py3-none-any.whl size=10291149 sha256=bed6549ebf65c1e8de6fed4b6c5a6079fdb00312fd166afa3d96be114be32030\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c1pto_up/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.1\n",
            "    Uninstalling transformers-4.46.1:\n",
            "      Successfully uninstalled transformers-4.46.1\n",
            "Successfully installed tokenizers-0.21.0 transformers-4.48.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e \".[torch, metrics]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLL6oZz2lyhT",
        "outputId": "f95333ca-180a-4015-f2d1-30f77149b82f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<=4.46.1,>=4.41.2 (from llamafactory==0.9.2.dev0)\n",
            "  Using cached transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: datasets<=3.1.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (3.1.0)\n",
            "Requirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.0.1)\n",
            "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.12.0)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.9.6)\n",
            "Collecting tokenizers<0.20.4,>=0.19.0 (from llamafactory==0.9.2.dev0)\n",
            "  Using cached tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: gradio<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (4.44.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.8.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (4.25.5)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.34.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.10.3)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.115.6)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.2.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (3.8.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.26.4)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (14.0.1)\n",
            "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.8.14)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.5.1+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (3.9.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.42.1)\n",
            "Collecting rouge-chinese (from llamafactory==0.9.2.dev0)\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.11.10)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (4.8.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.10.12)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (10.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.8.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.2.3)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (12.0)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.9.2.dev0) (0.41.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (2.27.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.2.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (1.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.2.dev0) (2.5.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->llamafactory==0.9.2.dev0) (1.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese->llamafactory==0.9.2.dev0) (1.17.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (0.1.2)\n",
            "Using cached tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Using cached transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
            "Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=23927 sha256=ad94b54e4c0a2b959336cd76be9edc6e31f642a6b1fc71e15057524e0c742788\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v0ihzhe7/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: rouge-chinese, tokenizers, transformers, llamafactory\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.0.dev0\n",
            "    Uninstalling transformers-4.48.0.dev0:\n",
            "      Successfully uninstalled transformers-4.48.0.dev0\n",
            "Successfully installed llamafactory-0.9.2.dev0 rouge-chinese-1.0.3 tokenizers-0.20.3 transformers-4.46.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install liger-kernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SZHnGDnmWXh",
        "outputId": "ca6a061d-8b14-4f8e-bdf3-c6509fca48e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting liger-kernel\n",
            "  Downloading liger_kernel-0.5.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from liger-kernel) (2.5.1+cu121)\n",
            "Collecting triton>=2.3.1 (from liger-kernel)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.2->liger-kernel) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.2->liger-kernel) (2.1.5)\n",
            "Downloading liger_kernel-0.5.2-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, liger-kernel\n",
            "Successfully installed liger-kernel-0.5.2 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fTOV0jfmmvT",
        "outputId": "8a890374-d461-4159-b11d-c27b8955a591"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-06 01:33:51.534691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-06 01:33:51.563609: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-06 01:33:51.572282: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-06 01:33:51.586588: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-06 01:33:52.628026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://101c6431b2135bc3d1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "2025-01-06 01:34:45.929257: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-06 01:34:45.948967: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-06 01:34:45.954687: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-06 01:34:46.998473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|2025-01-06 01:34:54] llamafactory.hparams.parser:359 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:34:54,658 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:34:54,659 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:54,757 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:54,757 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:54,757 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:54,757 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:54,757 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:54,758 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-06 01:34:55,111 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:34:55,677 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:34:55,678 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:55,846 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:55,846 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:55,846 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:55,846 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:55,847 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:34:55,847 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-06 01:34:56,193 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-01-06 01:34:56] llamafactory.data.template:157 >> Add <|im_end|> to stop words.\n",
            "[INFO|2025-01-06 01:34:56] llamafactory.data.loader:157 >> Loading dataset alpaca_zh_demo.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 1000 examples [00:00, 45910.13 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 1000/1000 [00:00<00:00, 1488.36 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:12<00:00, 78.35 examples/s] \n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 102450, 62926, 104136, 89012, 22382, 44177, 101047, 100369, 99891, 101911, 5122, 102150, 101911, 33108, 8903, 63109, 36587, 1773, 151645, 198, 151644, 77091, 198, 102150, 101911, 20412, 100206, 99891, 104111, 101911, 3837, 99652, 100140, 55338, 100702, 31914, 100132, 67071, 48934, 30709, 105166, 106251, 8545, 102150, 31838, 104384, 1773, 100346, 114651, 104111, 99896, 101911, 3837, 100140, 102150, 20412, 55338, 100206, 105166, 100166, 33108, 98380, 75317, 3837, 104152, 100206, 100132, 67071, 46944, 57191, 101213, 102150, 101286, 3837, 102150, 101097, 67338, 102150, 110935, 100394, 100676, 102150, 1773, 100147, 101911, 67071, 106929, 22382, 120806, 5373, 99330, 101190, 31843, 33108, 100167, 100809, 34204, 16, 23, 18, 24, 7948, 104181, 101080, 3407, 8903, 63109, 36587, 104442, 101281, 20412, 101281, 38176, 9370, 99488, 3837, 105884, 3837, 113837, 102074, 101281, 108215, 9370, 101911, 1773, 99487, 101911, 112479, 105062, 29490, 63109, 36587, 101313, 3837, 100140, 102493, 102095, 105339, 9370, 99488, 1773, 8903, 63109, 36587, 9370, 101080, 28946, 20412, 99685, 99470, 72225, 13935, 99826, 99243, 99685, 3837, 104677, 16, 21, 101186, 84607, 102098, 108124, 101712, 26940, 35727, 31914, 104001, 67831, 87243, 109268, 34187, 101281, 38176, 113837, 102074, 101281, 104001, 9370, 104949, 3837, 17714, 35727, 104179, 103949, 107759, 102334, 102007, 1773, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "识别并解释给定列表中的两个科学理论：细胞理论和日心说。<|im_end|>\n",
            "<|im_start|>assistant\n",
            "细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\n",
            "\n",
            "日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 102150, 101911, 20412, 100206, 99891, 104111, 101911, 3837, 99652, 100140, 55338, 100702, 31914, 100132, 67071, 48934, 30709, 105166, 106251, 8545, 102150, 31838, 104384, 1773, 100346, 114651, 104111, 99896, 101911, 3837, 100140, 102150, 20412, 55338, 100206, 105166, 100166, 33108, 98380, 75317, 3837, 104152, 100206, 100132, 67071, 46944, 57191, 101213, 102150, 101286, 3837, 102150, 101097, 67338, 102150, 110935, 100394, 100676, 102150, 1773, 100147, 101911, 67071, 106929, 22382, 120806, 5373, 99330, 101190, 31843, 33108, 100167, 100809, 34204, 16, 23, 18, 24, 7948, 104181, 101080, 3407, 8903, 63109, 36587, 104442, 101281, 20412, 101281, 38176, 9370, 99488, 3837, 105884, 3837, 113837, 102074, 101281, 108215, 9370, 101911, 1773, 99487, 101911, 112479, 105062, 29490, 63109, 36587, 101313, 3837, 100140, 102493, 102095, 105339, 9370, 99488, 1773, 8903, 63109, 36587, 9370, 101080, 28946, 20412, 99685, 99470, 72225, 13935, 99826, 99243, 99685, 3837, 104677, 16, 21, 101186, 84607, 102098, 108124, 101712, 26940, 35727, 31914, 104001, 67831, 87243, 109268, 34187, 101281, 38176, 113837, 102074, 101281, 104001, 9370, 104949, 3837, 17714, 35727, 104179, 103949, 107759, 102334, 102007, 1773, 151645]\n",
            "labels:\n",
            "细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\n",
            "\n",
            "日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|im_end|>\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:35:11,552 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:35:11,553 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3937] 2025-01-06 01:35:11,596 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/model.safetensors\n",
            "[INFO|modeling_utils.py:1670] 2025-01-06 01:35:11,607 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2025-01-06 01:35:11,609 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4800] 2025-01-06 01:35:12,439 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2025-01-06 01:35:12,439 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2025-01-06 01:35:12,683 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2025-01-06 01:35:12,684 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-01-06 01:35:12] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-01-06 01:35:12] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-01-06 01:35:12] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-01-06 01:35:12] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-01-06 01:35:12] llamafactory.model.model_utils.misc:157 >> Found linear modules: v_proj,q_proj,k_proj,gate_proj,down_proj,up_proj,o_proj\n",
            "[INFO|2025-01-06 01:35:13] llamafactory.model.loader:157 >> trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "[INFO|trainer.py:698] 2025-01-06 01:35:13,051 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2313] 2025-01-06 01:35:13,451 >> ***** Running training *****\n",
            "[INFO|trainer.py:2314] 2025-01-06 01:35:13,451 >>   Num examples = 1,000\n",
            "[INFO|trainer.py:2315] 2025-01-06 01:35:13,451 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2316] 2025-01-06 01:35:13,451 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2319] 2025-01-06 01:35:13,451 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2320] 2025-01-06 01:35:13,451 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2321] 2025-01-06 01:35:13,451 >>   Total optimization steps = 186\n",
            "[INFO|trainer.py:2322] 2025-01-06 01:35:13,455 >>   Number of trainable parameters = 4,399,104\n",
            "  3% 5/186 [00:26<15:35,  5.17s/it][INFO|2025-01-06 01:35:40] llamafactory.train.callbacks:157 >> {'loss': 1.7916, 'learning_rate': 4.9911e-05, 'epoch': 0.08}\n",
            "{'loss': 1.7916, 'grad_norm': 1.0145124197006226, 'learning_rate': 4.9910902453260824e-05, 'epoch': 0.08}\n",
            "  5% 10/186 [00:52<15:42,  5.36s/it][INFO|2025-01-06 01:36:06] llamafactory.train.callbacks:157 >> {'loss': 1.7590, 'learning_rate': 4.9644e-05, 'epoch': 0.16}\n",
            "{'loss': 1.759, 'grad_norm': 0.6721704602241516, 'learning_rate': 4.964424488287009e-05, 'epoch': 0.16}\n",
            "  8% 15/186 [01:18<15:02,  5.28s/it][INFO|2025-01-06 01:36:32] llamafactory.train.callbacks:157 >> {'loss': 1.9198, 'learning_rate': 4.9202e-05, 'epoch': 0.24}\n",
            "{'loss': 1.9198, 'grad_norm': 0.727702796459198, 'learning_rate': 4.920192797165511e-05, 'epoch': 0.24}\n",
            " 11% 20/186 [01:47<15:31,  5.61s/it][INFO|2025-01-06 01:37:00] llamafactory.train.callbacks:157 >> {'loss': 1.7207, 'learning_rate': 4.8587e-05, 'epoch': 0.32}\n",
            "{'loss': 1.7207, 'grad_norm': 0.7972226142883301, 'learning_rate': 4.858710446774951e-05, 'epoch': 0.32}\n",
            " 13% 25/186 [02:12<13:40,  5.10s/it][INFO|2025-01-06 01:37:25] llamafactory.train.callbacks:157 >> {'loss': 1.6653, 'learning_rate': 4.7804e-05, 'epoch': 0.40}\n",
            "{'loss': 1.6653, 'grad_norm': 0.9538538455963135, 'learning_rate': 4.780415671242334e-05, 'epoch': 0.4}\n",
            " 16% 30/186 [02:42<15:30,  5.97s/it][INFO|2025-01-06 01:37:55] llamafactory.train.callbacks:157 >> {'loss': 1.7586, 'learning_rate': 4.6859e-05, 'epoch': 0.48}\n",
            "{'loss': 1.7586, 'grad_norm': 0.6810439825057983, 'learning_rate': 4.685866540361456e-05, 'epoch': 0.48}\n",
            " 19% 35/186 [03:09<14:04,  5.59s/it][INFO|2025-01-06 01:38:23] llamafactory.train.callbacks:157 >> {'loss': 1.7486, 'learning_rate': 4.5757e-05, 'epoch': 0.56}\n",
            "{'loss': 1.7486, 'grad_norm': 0.8717197775840759, 'learning_rate': 4.5757369817809415e-05, 'epoch': 0.56}\n",
            " 22% 40/186 [03:36<12:52,  5.29s/it][INFO|2025-01-06 01:38:50] llamafactory.train.callbacks:157 >> {'loss': 1.7696, 'learning_rate': 4.4508e-05, 'epoch': 0.64}\n",
            "{'loss': 1.7696, 'grad_norm': 0.9800193905830383, 'learning_rate': 4.45081197738023e-05, 'epoch': 0.64}\n",
            " 24% 45/186 [04:04<13:06,  5.58s/it][INFO|2025-01-06 01:39:18] llamafactory.train.callbacks:157 >> {'loss': 1.7200, 'learning_rate': 4.3120e-05, 'epoch': 0.72}\n",
            "{'loss': 1.72, 'grad_norm': 0.8767461776733398, 'learning_rate': 4.3119819680728e-05, 'epoch': 0.72}\n",
            " 27% 50/186 [04:30<11:43,  5.17s/it][INFO|2025-01-06 01:39:44] llamafactory.train.callbacks:157 >> {'loss': 1.8712, 'learning_rate': 4.1602e-05, 'epoch': 0.80}\n",
            "{'loss': 1.8712, 'grad_norm': 0.9683165550231934, 'learning_rate': 4.160236506918098e-05, 'epoch': 0.8}\n",
            " 30% 55/186 [04:57<11:22,  5.21s/it][INFO|2025-01-06 01:40:11] llamafactory.train.callbacks:157 >> {'loss': 1.8052, 'learning_rate': 3.9967e-05, 'epoch': 0.88}\n",
            "{'loss': 1.8052, 'grad_norm': 0.8342147469520569, 'learning_rate': 3.9966572057815373e-05, 'epoch': 0.88}\n",
            " 32% 60/186 [05:24<11:12,  5.33s/it][INFO|2025-01-06 01:40:37] llamafactory.train.callbacks:157 >> {'loss': 1.7566, 'learning_rate': 3.8224e-05, 'epoch': 0.96}\n",
            "{'loss': 1.7566, 'grad_norm': 0.7861897349357605, 'learning_rate': 3.822410025817406e-05, 'epoch': 0.96}\n",
            " 35% 65/186 [05:52<11:16,  5.59s/it][INFO|2025-01-06 01:41:05] llamafactory.train.callbacks:157 >> {'loss': 1.7999, 'learning_rate': 3.6387e-05, 'epoch': 1.04}\n",
            "{'loss': 1.7999, 'grad_norm': 0.8083568811416626, 'learning_rate': 3.638736966726585e-05, 'epoch': 1.04}\n",
            " 38% 70/186 [06:17<09:54,  5.12s/it][INFO|2025-01-06 01:41:31] llamafactory.train.callbacks:157 >> {'loss': 1.7743, 'learning_rate': 3.4469e-05, 'epoch': 1.12}\n",
            "{'loss': 1.7743, 'grad_norm': 0.7677395939826965, 'learning_rate': 3.44694721402644e-05, 'epoch': 1.12}\n",
            " 40% 75/186 [06:44<09:51,  5.33s/it][INFO|2025-01-06 01:41:58] llamafactory.train.callbacks:157 >> {'loss': 1.6665, 'learning_rate': 3.2484e-05, 'epoch': 1.20}\n",
            "{'loss': 1.6665, 'grad_norm': 0.7315582633018494, 'learning_rate': 3.2484078074333954e-05, 'epoch': 1.2}\n",
            " 43% 80/186 [07:12<09:44,  5.52s/it][INFO|2025-01-06 01:42:26] llamafactory.train.callbacks:157 >> {'loss': 1.6776, 'learning_rate': 3.0445e-05, 'epoch': 1.28}\n",
            "{'loss': 1.6776, 'grad_norm': 0.9120276570320129, 'learning_rate': 3.0445338968721287e-05, 'epoch': 1.28}\n",
            " 46% 85/186 [07:40<09:24,  5.59s/it][INFO|2025-01-06 01:42:53] llamafactory.train.callbacks:157 >> {'loss': 1.6602, 'learning_rate': 2.8368e-05, 'epoch': 1.36}\n",
            "{'loss': 1.6602, 'grad_norm': 0.9768756031990051, 'learning_rate': 2.836778655564653e-05, 'epoch': 1.36}\n",
            " 48% 90/186 [08:10<09:41,  6.06s/it][INFO|2025-01-06 01:43:23] llamafactory.train.callbacks:157 >> {'loss': 1.8630, 'learning_rate': 2.6266e-05, 'epoch': 1.44}\n",
            "{'loss': 1.863, 'grad_norm': 0.8422000408172607, 'learning_rate': 2.6266229220967818e-05, 'epoch': 1.44}\n",
            " 51% 95/186 [08:36<08:09,  5.37s/it][INFO|2025-01-06 01:43:49] llamafactory.train.callbacks:157 >> {'loss': 1.6041, 'learning_rate': 2.4156e-05, 'epoch': 1.52}\n",
            "{'loss': 1.6041, 'grad_norm': 0.9062637090682983, 'learning_rate': 2.4155646452913296e-05, 'epoch': 1.52}\n",
            " 54% 100/186 [09:03<07:44,  5.40s/it][INFO|2025-01-06 01:44:17] llamafactory.train.callbacks:157 >> {'loss': 1.8217, 'learning_rate': 2.2051e-05, 'epoch': 1.60}\n",
            "{'loss': 1.8217, 'grad_norm': 0.7813428640365601, 'learning_rate': 2.2051082071228854e-05, 'epoch': 1.6}\n",
            " 54% 100/186 [09:03<07:44,  5.40s/it][INFO|trainer.py:3801] 2025-01-06 01:44:17,247 >> Saving model checkpoint to saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17/checkpoint-100\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:44:17,513 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:44:17,515 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2025-01-06 01:44:17,604 >> tokenizer config file saved in saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2025-01-06 01:44:17,604 >> Special tokens file saved in saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17/checkpoint-100/special_tokens_map.json\n",
            " 56% 105/186 [09:33<07:36,  5.64s/it][INFO|2025-01-06 01:44:46] llamafactory.train.callbacks:157 >> {'loss': 1.5943, 'learning_rate': 1.9968e-05, 'epoch': 1.68}\n",
            "{'loss': 1.5943, 'grad_norm': 0.9563556909561157, 'learning_rate': 1.9967536997783494e-05, 'epoch': 1.68}\n",
            " 59% 110/186 [10:00<07:04,  5.58s/it][INFO|2025-01-06 01:45:14] llamafactory.train.callbacks:157 >> {'loss': 1.7272, 'learning_rate': 1.7920e-05, 'epoch': 1.76}\n",
            "{'loss': 1.7272, 'grad_norm': 1.0136996507644653, 'learning_rate': 1.79198623329424e-05, 'epoch': 1.76}\n",
            " 62% 115/186 [10:29<06:34,  5.56s/it][INFO|2025-01-06 01:45:42] llamafactory.train.callbacks:157 >> {'loss': 1.6425, 'learning_rate': 1.5923e-05, 'epoch': 1.84}\n",
            "{'loss': 1.6425, 'grad_norm': 0.7430303692817688, 'learning_rate': 1.5922653499838137e-05, 'epoch': 1.84}\n",
            " 65% 120/186 [10:55<05:47,  5.27s/it][INFO|2025-01-06 01:46:09] llamafactory.train.callbacks:157 >> {'loss': 1.8761, 'learning_rate': 1.3990e-05, 'epoch': 1.92}\n",
            "{'loss': 1.8761, 'grad_norm': 1.0553174018859863, 'learning_rate': 1.399014621105914e-05, 'epoch': 1.92}\n",
            " 67% 125/186 [11:18<04:42,  4.64s/it][INFO|2025-01-06 01:46:31] llamafactory.train.callbacks:157 >> {'loss': 1.9285, 'learning_rate': 1.2136e-05, 'epoch': 2.00}\n",
            "{'loss': 1.9285, 'grad_norm': 2.119197368621826, 'learning_rate': 1.2136114999284288e-05, 'epoch': 2.0}\n",
            " 70% 130/186 [11:46<05:10,  5.55s/it][INFO|2025-01-06 01:46:59] llamafactory.train.callbacks:157 >> {'loss': 1.7622, 'learning_rate': 1.0374e-05, 'epoch': 2.08}\n",
            "{'loss': 1.7622, 'grad_norm': 0.8442848920822144, 'learning_rate': 1.0373775035117305e-05, 'epoch': 2.08}\n",
            " 73% 135/186 [12:15<04:45,  5.61s/it][INFO|2025-01-06 01:47:28] llamafactory.train.callbacks:157 >> {'loss': 1.7021, 'learning_rate': 8.7157e-06, 'epoch': 2.16}\n",
            "{'loss': 1.7021, 'grad_norm': 0.9630509614944458, 'learning_rate': 8.715687931944449e-06, 'epoch': 2.16}\n",
            " 75% 140/186 [12:42<04:03,  5.30s/it][INFO|2025-01-06 01:47:56] llamafactory.train.callbacks:157 >> {'loss': 1.7079, 'learning_rate': 7.1737e-06, 'epoch': 2.24}\n",
            "{'loss': 1.7079, 'grad_norm': 1.15211021900177, 'learning_rate': 7.173672209219495e-06, 'epoch': 2.24}\n",
            " 78% 145/186 [13:09<03:33,  5.20s/it][INFO|2025-01-06 01:48:22] llamafactory.train.callbacks:157 >> {'loss': 1.6827, 'learning_rate': 5.7587e-06, 'epoch': 2.32}\n",
            "{'loss': 1.6827, 'grad_norm': 1.0901315212249756, 'learning_rate': 5.758719052376693e-06, 'epoch': 2.32}\n",
            " 81% 150/186 [13:35<03:10,  5.30s/it][INFO|2025-01-06 01:48:49] llamafactory.train.callbacks:157 >> {'loss': 1.7252, 'learning_rate': 4.4809e-06, 'epoch': 2.40}\n",
            "{'loss': 1.7252, 'grad_norm': 0.9897742867469788, 'learning_rate': 4.480913969818098e-06, 'epoch': 2.4}\n",
            " 83% 155/186 [14:04<02:52,  5.58s/it][INFO|2025-01-06 01:49:17] llamafactory.train.callbacks:157 >> {'loss': 1.6693, 'learning_rate': 3.3494e-06, 'epoch': 2.48}\n",
            "{'loss': 1.6693, 'grad_norm': 0.9538813829421997, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.48}\n",
            " 86% 160/186 [14:29<02:17,  5.29s/it][INFO|2025-01-06 01:49:43] llamafactory.train.callbacks:157 >> {'loss': 1.6718, 'learning_rate': 2.3721e-06, 'epoch': 2.56}\n",
            "{'loss': 1.6718, 'grad_norm': 0.8550328016281128, 'learning_rate': 2.372137318741968e-06, 'epoch': 2.56}\n",
            " 89% 165/186 [14:57<01:52,  5.38s/it][INFO|2025-01-06 01:50:11] llamafactory.train.callbacks:157 >> {'loss': 1.5100, 'learning_rate': 1.5562e-06, 'epoch': 2.64}\n",
            "{'loss': 1.51, 'grad_norm': 0.9972015023231506, 'learning_rate': 1.5561966963229924e-06, 'epoch': 2.64}\n",
            " 91% 170/186 [15:23<01:24,  5.28s/it][INFO|2025-01-06 01:50:37] llamafactory.train.callbacks:157 >> {'loss': 1.8470, 'learning_rate': 9.0736e-07, 'epoch': 2.72}\n",
            "{'loss': 1.847, 'grad_norm': 0.9726325869560242, 'learning_rate': 9.073589027514789e-07, 'epoch': 2.72}\n",
            " 94% 175/186 [15:52<01:04,  5.83s/it][INFO|2025-01-06 01:51:06] llamafactory.train.callbacks:157 >> {'loss': 1.7060, 'learning_rate': 4.3025e-07, 'epoch': 2.80}\n",
            "{'loss': 1.706, 'grad_norm': 0.7792142033576965, 'learning_rate': 4.302487264785521e-07, 'epoch': 2.8}\n",
            " 97% 180/186 [16:19<00:31,  5.29s/it][INFO|2025-01-06 01:51:32] llamafactory.train.callbacks:157 >> {'loss': 1.6368, 'learning_rate': 1.2827e-07, 'epoch': 2.88}\n",
            "{'loss': 1.6368, 'grad_norm': 0.860988199710846, 'learning_rate': 1.2826691520262114e-07, 'epoch': 2.88}\n",
            " 99% 185/186 [16:45<00:05,  5.23s/it][INFO|2025-01-06 01:51:58] llamafactory.train.callbacks:157 >> {'loss': 1.6116, 'learning_rate': 3.5659e-09, 'epoch': 2.96}\n",
            "{'loss': 1.6116, 'grad_norm': 0.9328406453132629, 'learning_rate': 3.565936007254855e-09, 'epoch': 2.96}\n",
            "100% 186/186 [16:50<00:00,  5.27s/it][INFO|trainer.py:3801] 2025-01-06 01:52:04,225 >> Saving model checkpoint to saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17/checkpoint-186\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:52:04,452 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:52:04,453 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2025-01-06 01:52:04,502 >> tokenizer config file saved in saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17/checkpoint-186/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2025-01-06 01:52:04,502 >> Special tokens file saved in saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17/checkpoint-186/special_tokens_map.json\n",
            "[INFO|trainer.py:2584] 2025-01-06 01:52:04,801 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1011.3465, 'train_samples_per_second': 2.966, 'train_steps_per_second': 0.184, 'train_loss': 1.7348435764671655, 'epoch': 2.98}\n",
            "100% 186/186 [16:51<00:00,  5.44s/it]\n",
            "[INFO|trainer.py:3801] 2025-01-06 01:52:04,803 >> Saving model checkpoint to saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:52:05,010 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:52:05,011 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2025-01-06 01:52:05,060 >> tokenizer config file saved in saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2025-01-06 01:52:05,061 >> Special tokens file saved in saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      2.976\n",
            "  total_flos               =  1162285GF\n",
            "  train_loss               =     1.7348\n",
            "  train_runtime            = 0:16:51.34\n",
            "  train_samples_per_second =      2.966\n",
            "  train_steps_per_second   =      0.184\n",
            "Figure saved at: saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17/training_loss.png\n",
            "[WARNING|2025-01-06 01:52:05] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n",
            "[WARNING|2025-01-06 01:52:05] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
            "[INFO|modelcard.py:449] 2025-01-06 01:52:05,478 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:52:45,321 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:52:45,323 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:45,413 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:45,414 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:45,414 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:45,414 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:45,414 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:45,414 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-06 01:52:45,758 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:52:46,150 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:52:46,151 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:46,247 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:46,247 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:46,247 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:46,247 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:46,247 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:52:46,247 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-06 01:52:46,592 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-01-06 01:52:46] llamafactory.data.template:157 >> Add <|im_end|> to stop words.\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:52:46,713 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:52:46,714 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-01-06 01:52:46] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2025-01-06 01:52:46,755 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/model.safetensors\n",
            "[INFO|modeling_utils.py:1670] 2025-01-06 01:52:46,765 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2025-01-06 01:52:46,767 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4800] 2025-01-06 01:52:47,612 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2025-01-06 01:52:47,612 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2025-01-06 01:52:47,707 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2025-01-06 01:52:47,708 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-01-06 01:52:47] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-01-06 01:52:48] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2025-01-06 01:52:48] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17\n",
            "[INFO|2025-01-06 01:52:48] llamafactory.model.loader:157 >> all params: 494,032,768\n",
            "[WARNING|2025-01-06 01:52:48] llamafactory.chat.hf_engine:168 >> There is no current event loop, creating a new one.\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:54:32,670 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:54:32,671 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:32,769 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:32,769 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:32,769 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:32,769 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:32,769 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:32,769 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-06 01:54:33,111 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:54:33,570 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:54:33,571 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:33,692 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:33,692 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:33,692 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:33,692 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:33,692 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:54:33,692 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-06 01:54:34,034 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-01-06 01:54:34] llamafactory.data.template:157 >> Add <|im_end|> to stop words.\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:54:34,169 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:54:34,170 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-01-06 01:54:34] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2025-01-06 01:54:34,172 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/model.safetensors\n",
            "[INFO|modeling_utils.py:1670] 2025-01-06 01:54:34,184 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2025-01-06 01:54:34,186 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4800] 2025-01-06 01:54:35,005 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2025-01-06 01:54:35,005 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2025-01-06 01:54:35,125 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2025-01-06 01:54:35,125 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-01-06 01:54:35] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-01-06 01:54:35] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2025-01-06 01:54:35] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-00-44-28\n",
            "[INFO|2025-01-06 01:54:35] llamafactory.model.loader:157 >> all params: 494,032,768\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:55:02,191 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:55:02,192 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:02,284 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:02,284 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:02,284 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:02,284 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:02,285 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:02,285 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-06 01:55:02,612 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:55:02,998 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:55:03,000 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:03,091 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:03,091 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:03,091 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:03,091 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:03,091 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:55:03,091 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-06 01:55:03,431 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-01-06 01:55:03] llamafactory.data.template:157 >> Add <|im_end|> to stop words.\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:55:03,559 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:55:03,560 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-01-06 01:55:03] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2025-01-06 01:55:03,561 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/model.safetensors\n",
            "[INFO|modeling_utils.py:1670] 2025-01-06 01:55:03,571 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2025-01-06 01:55:03,572 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4800] 2025-01-06 01:55:04,428 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2025-01-06 01:55:04,429 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2025-01-06 01:55:04,519 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2025-01-06 01:55:04,519 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-01-06 01:55:04] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-01-06 01:55:04] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2025-01-06 01:55:04] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-03-37\n",
            "[INFO|2025-01-06 01:55:04] llamafactory.model.loader:157 >> all params: 494,032,768\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:59:05,458 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:59:05,459 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:05,551 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:05,551 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:05,551 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:05,551 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:05,552 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:05,552 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-06 01:59:06,062 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:59:06,470 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:59:06,472 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:06,565 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:06,565 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:06,565 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:06,565 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:06,565 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-06 01:59:06,565 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-06 01:59:06,899 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-01-06 01:59:06] llamafactory.data.template:157 >> Add <|im_end|> to stop words.\n",
            "[INFO|configuration_utils.py:679] 2025-01-06 01:59:07,029 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-06 01:59:07,030 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-01-06 01:59:07] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2025-01-06 01:59:07,031 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/model.safetensors\n",
            "[INFO|modeling_utils.py:1670] 2025-01-06 01:59:07,041 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2025-01-06 01:59:07,043 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4800] 2025-01-06 01:59:07,877 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2025-01-06 01:59:07,877 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2025-01-06 01:59:07,973 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2025-01-06 01:59:07,973 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-01-06 01:59:07] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-01-06 01:59:08] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2025-01-06 01:59:08] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Qwen2.5-0.5B-Instruct/lora/train_2025-01-06-01-34-17\n",
            "[INFO|2025-01-06 01:59:08] llamafactory.model.loader:157 >> all params: 494,032,768\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2709, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 116, in main\n",
            "    run_web_ui()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 93, in run_web_ui\n",
            "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2614, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2713, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/http_server.py\", line 68, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1100, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:7860 <> https://101c6431b2135bc3d1.gradio.live\n"
          ]
        }
      ]
    }
  ]
}